{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understanding the Data\n",
    "\n",
    "This notebook explains the two core data components in the active learning pipeline:\n",
    "1. **Embeddings**: Numerical representations of audio segments\n",
    "2. **Annotations**: Labels for each audio segment\n",
    "\n",
    "## What are embeddings?\n",
    "\n",
    "Embeddings are high-dimensional vectors that represent audio in numerical form. Instead of working with raw audio waveforms, we use pre-trained models (like BirdNET or Perch) to convert audio into fixed-size vectors (e.g., 1024 dimensions).\n",
    "\n",
    "Think of it like this:\n",
    "- Raw audio: 3 seconds of sound waves\n",
    "- Embedding: A list of 1024 numbers that \"describes\" that sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (22, 1024)\n",
      "\n",
      "This means:\n",
      "  - 22 audio segments (each 3 seconds long)\n",
      "  - 1024 dimensions per embedding\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Example: Load a single embedding file\n",
    "# These are stored as .npy (NumPy array) files\n",
    "embedding_file = Path(\"../../results/test_data/embeddings/2025-11-13_18-33___birdnet-test_data/audio/FewShot/CHE_01_20190101_163410_birdnet.npy\")\n",
    "\n",
    "if embedding_file.exists():\n",
    "    embeddings = np.load(embedding_file)\n",
    "    print(f\"Shape: {embeddings.shape}\")\n",
    "    print(f\"\\nThis means:\")\n",
    "    print(f\"  - {embeddings.shape[0]} audio segments (each 3 seconds long)\")\n",
    "    print(f\"  - {embeddings.shape[1]} dimensions per embedding\")\n",
    "else:\n",
    "    print(f\"File not found: {embedding_file}\")\n",
    "    print(\"Make sure you have embeddings generated first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Annotations\n",
    "\n",
    "Annotations tell us what species/class is in each audio segment. They're stored in CSV format with information about:\n",
    "- Which audio file\n",
    "- Start/end time of the segment\n",
    "- Label (species name or sound type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in annotation file:\n",
      "['start', 'end', 'audiofilename', 'label:default_classifier']\n",
      "\n",
      "Total annotations: 63\n",
      "\n",
      "First few rows:\n",
      "   start   end                             audiofilename  \\\n",
      "0    0.0   3.0  audio\\FewShot\\CHE_01_20190101_163410.wav   \n",
      "1    3.0   6.0  audio\\FewShot\\CHE_01_20190101_163410.wav   \n",
      "2    6.0   9.0  audio\\FewShot\\CHE_01_20190101_163410.wav   \n",
      "3    9.0  12.0  audio\\FewShot\\CHE_01_20190101_163410.wav   \n",
      "4   12.0  15.0  audio\\FewShot\\CHE_01_20190101_163410.wav   \n",
      "\n",
      "  label:default_classifier  \n",
      "0                Rock Wren  \n",
      "1       Clark's Nutcracker  \n",
      "2           Little Bustard  \n",
      "3      Yellow-tufted Pipit  \n",
      "4    White-crowned Sparrow  \n",
      "\n",
      "Unique labels:\n",
      "['Rock Wren' \"Clark's Nutcracker\" 'Little Bustard' 'Yellow-tufted Pipit'\n",
      " 'White-crowned Sparrow' \"Cassin's Finch\" 'Garden Warbler'\n",
      " 'Common Chaffinch' 'Flammulated Owl' 'Hazel Grouse' 'Common Cuckoo'\n",
      " 'Western Orphean Warbler' 'Eurasian Three-toed Woodpecker'\n",
      " 'Lesser Nighthawk' 'Cinnamon Flycatcher' 'Black-billed Mountain-Toucan'\n",
      " 'Eurasian Blackbird' 'Chinese Blackbird' 'Eurasian Blue Tit'\n",
      " 'Common Redstart' 'Superb Lyrebird' 'Eurasian Wren' 'Purple Sunbird']\n"
     ]
    }
   ],
   "source": [
    "# Load annotations\n",
    "annotations_file = Path(\"../../results/test_data/evaluations/birdnet/classification/default_classifier_annotations.csv\")\n",
    "\n",
    "if annotations_file.exists():\n",
    "    df = pd.read_csv(annotations_file)\n",
    "    print(\"Columns in annotation file:\")\n",
    "    print(df.columns.tolist())\n",
    "    print(f\"\\nTotal annotations: {len(df)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nUnique labels:\")\n",
    "    print(df['label:default_classifier'].unique())\n",
    "else:\n",
    "    print(f\"File not found: {annotations_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Embeddings to Annotations\n",
    "\n",
    "The key challenge: We need to match each annotation (with start/end time) to the correct embedding segment.\n",
    "\n",
    "Here's how it works:\n",
    "1. Audio file is divided into 3-second segments\n",
    "2. Each segment gets an embedding (index 0, 1, 2, ...)\n",
    "3. Annotations have start times (0s, 3s, 6s, ...)\n",
    "4. We calculate: `segment_index = start_time / 3.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation: Rock Wren at 0.0s\n",
      "Corresponds to segment index: 0\n",
      "\n",
      "Embedding for this segment:\n",
      "Shape: (1024,)\n"
     ]
    }
   ],
   "source": [
    "# Example: Match an annotation to its embedding\n",
    "if annotations_file.exists() and embedding_file.exists():\n",
    "    # Get first annotation\n",
    "    row = df.iloc[0]\n",
    "    start_time = row['start']\n",
    "    label = row['label:default_classifier']\n",
    "    \n",
    "    # Calculate which segment this is\n",
    "    segment_duration = 3.0\n",
    "    segment_idx = int(start_time // segment_duration)\n",
    "    \n",
    "    print(f\"Annotation: {label} at {start_time}s\")\n",
    "    print(f\"Corresponds to segment index: {segment_idx}\")\n",
    "    print(f\"\\nEmbedding for this segment:\")\n",
    "    print(f\"Shape: {embeddings[segment_idx].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Multiple Audio Files\n",
    "\n",
    "In practice, you have many audio files, not just one. The key challenge is keeping track of which embedding belongs to which annotation across all files.\n",
    "\n",
    "Let's see how this works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Data Flow with Multiple Files:**\n",
    "```\n",
    "Audio Files (multiple .wav files)\n",
    "    ↓\n",
    "Pre-trained Model (BirdNET)\n",
    "    ↓\n",
    "Embedding Files (multiple .npy files, one per audio file)\n",
    "    +\n",
    "Annotations File (single .csv with all annotations)\n",
    "    ↓\n",
    "Matching Algorithm (iterate annotations, find corresponding embeddings)\n",
    "    ↓\n",
    "Flattened Dataset: Single array of (embedding, label) pairs\n",
    "    ↓\n",
    "Active Learning: Work with indices 0, 1, 2, ..., N-1\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "1. **One embedding file per audio file** - segments are stored as rows in the array\n",
    "2. **One annotations file for all audio** - contains all segments from all files\n",
    "3. **Matching by filename + time** - convert filename and use time to get segment index\n",
    "4. **Flatten to single dataset** - lose file structure, gain simple indexing\n",
    "5. **Track with indices** - active learning just needs 0, 1, 2, ..., N-1\n",
    "\n",
    "**Visual Example:**\n",
    "```\n",
    "File 1: CHE_01.wav (22 segments) → Dataset indices [0-21]\n",
    "File 2: KEN_01.wav (18 segments) → Dataset indices [22-39]  \n",
    "File 3: TAM_01.wav (23 segments) → Dataset indices [40-62]\n",
    "                                    Total: 63 samples\n",
    "```\n",
    "\n",
    "Now when active learning picks \"sample 25\", it automatically gets the right embedding from File 2, segment 3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is What ActiveLearner Does\n",
    "\n",
    "The `ActiveLearner._load_data()` method does exactly this process:\n",
    "\n",
    "```python\n",
    "def _load_data(self):\n",
    "    # Load annotations\n",
    "    df = pd.read_csv(self.annotations_path)\n",
    "    \n",
    "    embeddings_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Loop through each annotation\n",
    "    for _, row in df.iterrows():\n",
    "        audio_filename = row['audiofilename']\n",
    "        label = row['label:default_classifier']\n",
    "        \n",
    "        # Convert to embedding filename\n",
    "        filename_parts = Path(audio_filename).stem\n",
    "        embedding_filename = f\"{filename_parts}_{self.model_name}.npy\"\n",
    "        embedding_path = self.embeddings_dir / embedding_filename\n",
    "        \n",
    "        if embedding_path.exists():\n",
    "            # Load embedding file\n",
    "            emb = np.load(embedding_path)\n",
    "            \n",
    "            # Calculate segment index\n",
    "            start_time = row['start']\n",
    "            segment_duration = 3.0\n",
    "            segment_idx = int(start_time / segment_duration)\n",
    "            \n",
    "            if segment_idx < len(emb):\n",
    "                # Add this specific segment\n",
    "                embeddings_list.append(emb[segment_idx])\n",
    "                labels_list.append(label_to_idx[label])\n",
    "    \n",
    "    # Return flattened arrays\n",
    "    return np.array(embeddings_list), np.array(labels_list), ...\n",
    "```\n",
    "\n",
    "The result is two parallel arrays:\n",
    "- `self.embeddings`: shape (N, 1024)\n",
    "- `self.labels`: shape (N,)\n",
    "\n",
    "Where N is the total number of matched segments across ALL files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Structure:\n",
      "  Audio File 1 → [segment 0, segment 1, segment 2, ...]\n",
      "  Audio File 2 → [segment 0, segment 1, segment 2, ...]\n",
      "  Audio File 3 → [segment 0, segment 1, segment 2, ...]\n",
      "\n",
      "Flattened for Active Learning:\n",
      "  Dataset → [sample 0, sample 1, sample 2, ..., sample N-1]\n",
      "\n",
      "Where each sample is a (embedding, label) pair from ANY file\n",
      "\n",
      "Example mapping:\n",
      "  Dataset index 0 ← CHE_01_20190101_163410.wav segment 0\n",
      "  Dataset index 5 ← CHE_01_20190101_163410.wav segment 5\n",
      "  Dataset index 22 ← KEN_01_20190101_121500.wav segment 0\n",
      "  Dataset index 30 ← KEN_01_20190101_121500.wav segment 8\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the flattening concept\n",
    "print(\"Original Structure:\")\n",
    "print(\"  Audio File 1 → [segment 0, segment 1, segment 2, ...]\")\n",
    "print(\"  Audio File 2 → [segment 0, segment 1, segment 2, ...]\")\n",
    "print(\"  Audio File 3 → [segment 0, segment 1, segment 2, ...]\")\n",
    "print()\n",
    "print(\"Flattened for Active Learning:\")\n",
    "print(\"  Dataset → [sample 0, sample 1, sample 2, ..., sample N-1]\")\n",
    "print()\n",
    "print(\"Where each sample is a (embedding, label) pair from ANY file\")\n",
    "print()\n",
    "print(\"Example mapping:\")\n",
    "if 'metadata' in dir() and metadata and len(metadata) >= 10:\n",
    "    for i in [0, 5, 10]:\n",
    "        if i < len(metadata):\n",
    "            meta = metadata[i]\n",
    "            print(f\"  Dataset index {i} ← {Path(meta['audio_file']).name} segment {meta['segment_idx']}\")\n",
    "else:\n",
    "    print(\"  Dataset index 0 ← CHE_01_20190101_163410.wav segment 0\")\n",
    "    print(\"  Dataset index 5 ← CHE_01_20190101_163410.wav segment 5\")\n",
    "    print(\"  Dataset index 22 ← KEN_01_20190101_121500.wav segment 0\")\n",
    "    print(\"  Dataset index 30 ← KEN_01_20190101_121500.wav segment 8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insight: Flattening the Data\n",
    "\n",
    "Notice what happened:\n",
    "- We started with **multiple audio files**, each with **multiple segments**\n",
    "- We created a **flat list** where each entry is a single (embedding, label) pair\n",
    "- The original file structure is \"forgotten\" - we just have indices 0, 1, 2, ..., N-1\n",
    "\n",
    "**Why?** For active learning, we don't care which file a sample came from. We just need:\n",
    "- A pool of embeddings to train on\n",
    "- Their corresponding labels\n",
    "- Indices to track which are labeled vs unlabeled\n",
    "\n",
    "The `metadata` list lets us trace back to the original file if needed for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built dataset with 33 samples\n",
      "Embeddings shape: (33, 1024)\n",
      "\n",
      "First 5 samples metadata:\n",
      "  [0] CHE_01_20190101_163410.wav segment 0 @ 0.0s → Rock Wren\n",
      "  [1] CHE_01_20190101_163410.wav segment 1 @ 3.0s → Clark's Nutcracker\n",
      "  [2] CHE_01_20190101_163410.wav segment 2 @ 6.0s → Little Bustard\n",
      "  [3] CHE_01_20190101_163410.wav segment 3 @ 9.0s → Yellow-tufted Pipit\n",
      "  [4] CHE_01_20190101_163410.wav segment 4 @ 12.0s → White-crowned Sparrow\n"
     ]
    }
   ],
   "source": [
    "# Simulate the matching process\n",
    "def build_dataset(annotations_df, embeddings_dir, model_name=\"birdnet\"):\n",
    "    \"\"\"\n",
    "    Build a dataset by matching annotations to embeddings across multiple files\n",
    "    \n",
    "    Returns:\n",
    "        embeddings_list: List of embeddings\n",
    "        labels_list: List of corresponding labels\n",
    "        metadata: List of dicts with file info for each sample\n",
    "    \"\"\"\n",
    "    embeddings_list = []\n",
    "    labels_list = []\n",
    "    metadata = []\n",
    "    \n",
    "    for idx, row in annotations_df.iterrows():\n",
    "        # Get annotation info\n",
    "        audio_filename = row['audiofilename']\n",
    "        start_time = row['start']\n",
    "        label = row['label:default_classifier']\n",
    "        \n",
    "        # Convert to embedding filename\n",
    "        audio_stem = Path(audio_filename).stem  # Remove .wav extension\n",
    "        embedding_filename = f\"{audio_stem}_{model_name}.npy\"\n",
    "        embedding_path = embeddings_dir / embedding_filename\n",
    "        \n",
    "        # Load embedding file if it exists\n",
    "        if embedding_path.exists():\n",
    "            embeddings_array = np.load(embedding_path)\n",
    "            \n",
    "            # Calculate segment index\n",
    "            segment_duration = 3.0\n",
    "            segment_idx = int(start_time / segment_duration)\n",
    "            \n",
    "            # Check if segment exists in the embedding file\n",
    "            if segment_idx < len(embeddings_array):\n",
    "                # Extract the specific segment's embedding\n",
    "                embedding = embeddings_array[segment_idx]\n",
    "                \n",
    "                # Add to our dataset\n",
    "                embeddings_list.append(embedding)\n",
    "                labels_list.append(label)\n",
    "                metadata.append({\n",
    "                    'audio_file': audio_filename,\n",
    "                    'start_time': start_time,\n",
    "                    'segment_idx': segment_idx,\n",
    "                    'dataset_idx': len(embeddings_list) - 1\n",
    "                })\n",
    "    \n",
    "    return np.array(embeddings_list), labels_list, metadata\n",
    "\n",
    "# Build the dataset\n",
    "if annotations_file.exists() and embedding_file.exists():\n",
    "    embeddings_dir = Path(\"../../results/test_data/embeddings/2025-11-13_18-33___birdnet-test_data/audio/FewShot\")\n",
    "    if embeddings_dir.exists():\n",
    "        embeddings_array, labels_list, metadata = build_dataset(\n",
    "            df, \n",
    "            embeddings_dir,\n",
    "            model_name=\"birdnet\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Built dataset with {len(embeddings_array)} samples\")\n",
    "        print(f\"Embeddings shape: {embeddings_array.shape}\")\n",
    "        print(f\"\\nFirst 5 samples metadata:\")\n",
    "        for i in range(min(5, len(metadata))):\n",
    "            meta = metadata[i]\n",
    "            print(f\"  [{meta['dataset_idx']}] {Path(meta['audio_file']).name} \"\n",
    "                  f\"segment {meta['segment_idx']} @ {meta['start_time']}s → {labels_list[i]}\")\n",
    "    else:\n",
    "        print(\"Embeddings directory not found\")\n",
    "else:\n",
    "    print(\"Simulating with example output...\")\n",
    "    print(\"Built dataset with 63 samples\")\n",
    "    print(\"Embeddings shape: (63, 1024)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Matching Algorithm\n",
    "\n",
    "Here's the complete process for building the dataset across multiple files:\n",
    "\n",
    "1. **Loop through each annotation** in the CSV\n",
    "2. **Extract the audio filename** from the annotation\n",
    "3. **Convert audio filename to embedding filename**\n",
    "   - Example: `CHE_01_20190101_163410.wav` → `CHE_01_20190101_163410_birdnet.npy`\n",
    "4. **Load the embedding file** for that audio\n",
    "5. **Calculate segment index** from start time: `segment_idx = start_time / 3.0`\n",
    "6. **Extract the specific embedding** for that segment\n",
    "7. **Pair it with the label** from the annotation\n",
    "8. **Append to dataset**\n",
    "\n",
    "This creates a flat list of (embedding, label) pairs from across all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique audio files: 7\n",
      "\n",
      "Audio files in dataset:\n",
      "  CHE_01_20190101_163410.wav: 22 segments\n",
      "  CHE_02_20190101_183410.wav: 4 segments\n",
      "  CHE_03_20190201_163410.wav: 3 segments\n",
      "  CHE_04_20190203_175410.wav: 4 segments\n",
      "  242A2604603691DD_20250503_031300.WAV: 10 segments\n",
      "  ... and 2 more files\n"
     ]
    }
   ],
   "source": [
    "# Show all audio files in the dataset\n",
    "if annotations_file.exists():\n",
    "    unique_files = df['audiofilename'].unique()\n",
    "    print(f\"Total unique audio files: {len(unique_files)}\")\n",
    "    print(f\"\\nAudio files in dataset:\")\n",
    "    for audio_file in unique_files[:5]:  # Show first 5\n",
    "        file_annotations = df[df['audiofilename'] == audio_file]\n",
    "        print(f\"  {Path(audio_file).name}: {len(file_annotations)} segments\")\n",
    "\n",
    "    if len(unique_files) > 5:\n",
    "        print(f\"  ... and {len(unique_files) - 5} more files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "active",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
