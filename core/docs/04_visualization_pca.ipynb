{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualization with PCA\n",
    "\n",
    "This notebook explains how we visualize high-dimensional embeddings in 3D using PCA.\n",
    "\n",
    "## The Problem: Too Many Dimensions\n",
    "\n",
    "Our embeddings have **1024 dimensions**. Humans can't visualize 1024-dimensional space!\n",
    "\n",
    "We need to reduce to **3 dimensions** (x, y, z) for 3D visualization.\n",
    "\n",
    "## Solution: PCA (Principal Component Analysis)\n",
    "\n",
    "PCA finds the most important directions in high-dimensional data and projects it down.\n",
    "\n",
    "Think of it like finding the \"best camera angle\" to view your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create fake high-dimensional data\n",
    "n_samples = 100\n",
    "n_features = 1024  # Like our embeddings\n",
    "\n",
    "# Generate data with some structure\n",
    "# Class 0: centered at origin\n",
    "# Class 1: shifted in random direction\n",
    "# Class 2: shifted in another direction\n",
    "np.random.seed(42)\n",
    "class_0 = np.random.randn(30, n_features) * 0.5\n",
    "class_1 = np.random.randn(35, n_features) * 0.5 + 2\n",
    "class_2 = np.random.randn(35, n_features) * 0.5 - 2\n",
    "\n",
    "embeddings = np.vstack([class_0, class_1, class_2])\n",
    "labels = np.array([0]*30 + [1]*35 + [2]*35)\n",
    "\n",
    "print(f\"Data shape: {embeddings.shape}\")\n",
    "print(f\"Labels: {labels[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Standardization\n",
    "\n",
    "Before PCA, we standardize the features (mean=0, std=1).\n",
    "\n",
    "**Why?** PCA is sensitive to scale. If one feature has values 0-1000 and another 0-1, PCA will focus on the large-scale feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "\n",
    "print(\"Before scaling:\")\n",
    "print(f\"  Mean: {embeddings.mean():.4f}\")\n",
    "print(f\"  Std: {embeddings.std():.4f}\")\n",
    "\n",
    "print(\"\\nAfter scaling:\")\n",
    "print(f\"  Mean: {embeddings_scaled.mean():.4f}\")\n",
    "print(f\"  Std: {embeddings_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Apply PCA\n",
    "\n",
    "PCA finds the top 3 principal components - the 3 directions that capture the most variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 3D\n",
    "pca = PCA(n_components=3)\n",
    "embeddings_3d = pca.fit_transform(embeddings_scaled)\n",
    "\n",
    "print(f\"Reduced shape: {embeddings_3d.shape}\")\n",
    "print(f\"\\nVariance explained by each component:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.2%}\")\n",
    "print(f\"\\nTotal variance explained: {pca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- If total variance > 80%, the 3D projection is a good representation\n",
    "- If total variance < 50%, we're losing a lot of information\n",
    "\n",
    "For our random data, variance will be low. For real embeddings with structure, it's usually higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize in 3D\n",
    "\n",
    "Now we can plot our 3D coordinates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Color by class\n",
    "colors = ['red', 'blue', 'green']\n",
    "for class_id in range(3):\n",
    "    mask = labels == class_id\n",
    "    ax.scatter(\n",
    "        embeddings_3d[mask, 0],\n",
    "        embeddings_3d[mask, 1],\n",
    "        embeddings_3d[mask, 2],\n",
    "        c=colors[class_id],\n",
    "        label=f'Class {class_id}',\n",
    "        s=50,\n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title('3D PCA Projection of Embeddings')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"If classes cluster together, our embeddings are good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PCA with Trained Model Embeddings\n",
    "\n",
    "In our active learning pipeline, we:\n",
    "1. Train the model on labeled data\n",
    "2. Extract intermediate embeddings from the model\n",
    "3. Apply PCA to reduce to 3D\n",
    "4. Visualize to see how well the model separates classes\n",
    "\n",
    "**Important:** We fit PCA once and reuse the transformation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating model embeddings at different training stages\n",
    "def simulate_model_embeddings(iteration):\n",
    "    \"\"\"Simulate embeddings that cluster better over time\"\"\"\n",
    "    noise_level = 1.0 / (iteration + 1)  # Less noise as training progresses\n",
    "    \n",
    "    class_0 = np.random.randn(30, 512) * noise_level\n",
    "    class_1 = np.random.randn(35, 512) * noise_level + 3\n",
    "    class_2 = np.random.randn(35, 512) * noise_level - 3\n",
    "    \n",
    "    return np.vstack([class_0, class_1, class_2])\n",
    "\n",
    "# Fit PCA on initial embeddings\n",
    "initial_embeddings = simulate_model_embeddings(0)\n",
    "scaler = StandardScaler()\n",
    "initial_scaled = scaler.fit_transform(initial_embeddings)\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(initial_scaled)\n",
    "\n",
    "print(\"Fitted PCA transformation\")\n",
    "print(\"Now we can reuse this transformation for all iterations...\\n\")\n",
    "\n",
    "# Visualize progression\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "iterations = [0, 2, 5]\n",
    "\n",
    "for idx, iteration in enumerate(iterations):\n",
    "    # Get embeddings for this iteration\n",
    "    emb = simulate_model_embeddings(iteration)\n",
    "    \n",
    "    # Transform using SAME PCA\n",
    "    emb_scaled = scaler.transform(emb)\n",
    "    emb_3d = pca.transform(emb_scaled)\n",
    "    \n",
    "    # Plot\n",
    "    ax = fig.add_subplot(1, 3, idx + 1, projection='3d')\n",
    "    for class_id in range(3):\n",
    "        mask = labels == class_id\n",
    "        ax.scatter(\n",
    "            emb_3d[mask, 0],\n",
    "            emb_3d[mask, 1],\n",
    "            emb_3d[mask, 2],\n",
    "            c=colors[class_id],\n",
    "            s=30,\n",
    "            alpha=0.6\n",
    "        )\n",
    "    ax.set_title(f'Iteration {iteration}')\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.set_zlabel('PC3')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how classes cluster better as iterations progress!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**PCA Workflow:**\n",
    "1. **Standardize** high-dimensional data (mean=0, std=1)\n",
    "2. **Fit PCA** to find principal components\n",
    "3. **Transform** to 3D coordinates\n",
    "4. **Visualize** to understand clustering\n",
    "\n",
    "**In our pipeline:**\n",
    "- We fit PCA once on initial model embeddings\n",
    "- We reuse the same transformation for all subsequent iterations\n",
    "- This ensures consistent visualization as the model trains\n",
    "\n",
    "**What to look for:**\n",
    "- âœ… Classes forming distinct clusters = model is learning\n",
    "- âŒ All mixed together = model needs more training/data\n",
    "- ðŸ”„ Clusters getting tighter over time = active learning is working!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
