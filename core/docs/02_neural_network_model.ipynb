{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Neural Network Model\n",
    "\n",
    "This notebook explains the `EmbeddingClassifier` model - a simple 2-layer neural network that:\n",
    "1. Projects embeddings to a new space (optional)\n",
    "2. Classifies them into species/categories\n",
    "\n",
    "## Why do we need a model?\n",
    "\n",
    "We already have embeddings from BirdNET/Perch, so why train another model?\n",
    "\n",
    "**Answer:** The pre-trained embeddings are general-purpose. We want to:\n",
    "- Adapt them to OUR specific dataset\n",
    "- Learn which features matter most for OUR classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "The model consist of two customisable components 1) A **projection layer** which projects feature embeddings to a new embedding of the same dimension (this is visualising learning, the feature embedding doesn't change), 2) A **classification head** with outputs corresponding to the number of classes.\n",
    "\n",
    "```\n",
    "Input Embedding (1024 dims)\n",
    "        ↓\n",
    "   [Linear Layer] ← projection layer\n",
    "        ↓\n",
    "Hidden Embedding (1024 dims or custom)\n",
    "        ↓\n",
    "   [Linear Layer] ← classification head\n",
    "        ↓\n",
    "Class Logits (23 classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingClassifier(\n",
      "  (projection): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (classifier): Linear(in_features=512, out_features=23, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 536,599\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import our model\n",
    "sys.path.insert(0, str(Path().absolute().parent.parent))\n",
    "from core.model import EmbeddingClassifier\n",
    "\n",
    "# Create a model instance\n",
    "model = EmbeddingClassifier(\n",
    "    input_dim=1024,    # BirdNET embedding size\n",
    "    hidden_dim=512,    # Reduce to 512 dims (optional)\n",
    "    num_classes=23     # Number of bird species\n",
    ")\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Model Works\n",
    "\n",
    "Let's trace what happens to a single embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1024])\n",
      "Output shape: torch.Size([1, 23])\n",
      "\n",
      "These are 'logits' - raw scores for each of 23 classes\n",
      "First 5 logits: tensor([-0.2530,  0.0870,  0.2666,  2.8586,  0.6068])\n"
     ]
    }
   ],
   "source": [
    "# Create a fake embedding (batch_size=1, features=1024)\n",
    "fake_embedding = torch.randn(1, 1024)\n",
    "\n",
    "print(\"Input shape:\", fake_embedding.shape)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():  # Don't compute gradients for this demo\n",
    "    logits = model(fake_embedding)\n",
    "    \n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(f\"\\nThese are 'logits' - raw scores for each of {logits.shape[1]} classes\")\n",
    "print(\"First 5 logits:\", logits[0, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Logits to Probabilities\n",
    "\n",
    "Logits are raw scores. To get probabilities (that sum to 1), we use **softmax**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities shape: torch.Size([1, 23])\n",
      "Sum of probabilities: 0.9999999403953552\n",
      "\n",
      "Top 5 class probabilities:\n",
      "  Class 3: 0.3044\n",
      "  Class 9: 0.1671\n",
      "  Class 22: 0.0641\n",
      "  Class 18: 0.0629\n",
      "  Class 15: 0.0601\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(fake_embedding)\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    \n",
    "print(\"Probabilities shape:\", probabilities.shape)\n",
    "print(\"Sum of probabilities:\", probabilities.sum().item())\n",
    "print(\"\\nTop 5 class probabilities:\")\n",
    "top5_probs, top5_classes = torch.topk(probabilities, 5)\n",
    "for i in range(5):\n",
    "    print(f\"  Class {top5_classes[0, i].item()}: {top5_probs[0, i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Intermediate Embedding\n",
    "\n",
    "The model has a special method to extract the hidden/intermediate embedding. This is useful for:\n",
    "- Visualization (we can reduce 512 dims to 3D using PCA)\n",
    "- Understanding what the model learnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hidden_embedding = model.get_embedding(fake_embedding)\n",
    "    \n",
    "print(\"Hidden embedding shape:\", hidden_embedding.shape)\n",
    "print(\"This is the 'learned' representation of our input\")\n",
    "print(\"\\nFirst 10 values:\", hidden_embedding[0, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Training happens via standard supervised learning:\n",
    "1. Forward pass: Get predictions\n",
    "2. Compute loss: Compare predictions to true labels\n",
    "3. Backward pass: Calculate gradients\n",
    "4. Update weights: Adjust model parameters\n",
    "\n",
    "Here's a minimal training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fake training data\n",
    "batch_size = 8\n",
    "X_batch = torch.randn(batch_size, 1024)  # 8 embeddings\n",
    "y_batch = torch.randint(0, 23, (batch_size,))  # 8 random labels (0-22)\n",
    "\n",
    "# Setup training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Single training step\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward\n",
    "logits = model(X_batch)\n",
    "loss = criterion(logits, y_batch)\n",
    "\n",
    "# Backward\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    accuracy = (predictions == y_batch).float().mean()\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(f\"Accuracy: {accuracy.item():.4f}\")\n",
    "print(f\"\\nPredictions: {predictions.tolist()}\")\n",
    "print(f\"True labels:  {y_batch.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**The model does 3 things:**\n",
    "1. **Projects** input embeddings to a (possibly smaller) hidden space\n",
    "2. **Classifies** hidden embeddings into species categories\n",
    "3. **Learns** which features are important through training\n",
    "\n",
    "**Key insight:** As we add more labeled data through active learning, this model gets better at:\n",
    "- Distinguishing between species\n",
    "- Creating meaningful embeddings\n",
    "- Generalizing to unlabeled data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "active",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
